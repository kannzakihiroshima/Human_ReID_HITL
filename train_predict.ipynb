{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f03004-a85f-44c8-bc16-096a1d76f766",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_metric_learning import distances\n",
    "from pytorch_metric_learning.losses import ArcFaceLoss\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# ◆ WeightedArcFaceLoss (ArcFaceLoss の継承)\n",
    "# --------------------------------------------------\n",
    "class WeightedArcFaceLoss(ArcFaceLoss):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes,\n",
    "            embedding_size,\n",
    "            margin=28.6,\n",
    "            scale=64,\n",
    "            class_weights=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_classes=num_classes,\n",
    "            embedding_size=embedding_size,\n",
    "            margin=margin,\n",
    "            scale=scale,\n",
    "            **kwargs\n",
    "        )\n",
    "        if class_weights is not None:\n",
    "            if not isinstance(class_weights, torch.Tensor):\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(\n",
    "            weight=class_weights,\n",
    "            reduction=\"none\"\n",
    "        )\n",
    "\n",
    "\n",
    "def map_pids_to_0_known(df, known_pids, label_col=\"pid\", new_label_col=\"pid_trans\"):\n",
    "    \"\"\"\n",
    "    既知のpidがあれば label = i+1 (1から開始),\n",
    "    それ以外はラベル0とする\n",
    "    \"\"\"\n",
    "    sorted_kps = sorted(known_pids)\n",
    "    pid_map = {p: i + 1 for i, p in enumerate(sorted_kps)}\n",
    "    df[new_label_col] = df[label_col].apply(lambda x: pid_map.get(x, 0))\n",
    "    return df\n",
    "\n",
    "\n",
    "def downsample_label0(df, label_col=\"pid\"):\n",
    "    \"\"\"\n",
    "    label=0(未知)のサンプルが多すぎるのを抑制する\n",
    "    \"\"\"\n",
    "    label0_df = df[df[label_col] == 0]\n",
    "    others_df = df[df[label_col] != 0]\n",
    "    counts = others_df[label_col].value_counts()\n",
    "    max_count = counts.max() if not counts.empty else 0\n",
    "    max_count = max(max_count * 30, 128)\n",
    "    print(\"各ラベルのサンプル数:\")\n",
    "    print(df[label_col].value_counts())\n",
    "    print(f\"→ ラベル0を {max_count} 枚に合わせます\")\n",
    "    if len(label0_df) > max_count:\n",
    "        label0_df = label0_df.sample(n=max_count, random_state=42)\n",
    "    new_df = pd.concat([label0_df, others_df], ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def set_random_seed(seed_value=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def extract_features_and_labels(df):\n",
    "    \"\"\"\n",
    "    df内に 'embedding' 列があればそこから (N,dim) をstack,\n",
    "    なければ dim0..dimN を探す\n",
    "    ラベルは pid_trans があればそれ、なければ pid\n",
    "    \"\"\"\n",
    "    if \"embedding\" in df.columns:\n",
    "        X = np.stack(df[\"embedding\"].values)\n",
    "    else:\n",
    "        feature_cols = [c for c in df.columns if c.startswith(\"dim\")]\n",
    "        X = df[feature_cols].values\n",
    "\n",
    "    if \"pid_trans\" in df.columns:\n",
    "        y = df[\"pid_trans\"].values\n",
    "    else:\n",
    "        y = df[\"pid\"].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class Net128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net128, self).__init__()\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(512, 384)\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(384, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loss_func, device, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for features, labels in loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(features)\n",
    "        loss_output = loss_func(embeddings, labels)\n",
    "        if isinstance(loss_output, dict):\n",
    "            loss_val = loss_output[\"loss\"][\"losses\"].mean()\n",
    "        else:\n",
    "            loss_val = loss_output\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss_val.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def train_net128_weighted_arcface(X_train, y_train, device, epochs=100, lr=1e-6, batch_size=128):\n",
    "    \"\"\"\n",
    "    ArcFaceを重み付きCrossEntropyで学習する (デフォルトでは epochs=100, lr=1e-6)\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    unique_labels = np.unique(y_train)\n",
    "    num_classes = unique_labels.max() + 1\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    model = Net128().to(device)\n",
    "\n",
    "    class_weights = np.zeros(num_classes, dtype=np.float32)\n",
    "    for lab in unique_labels:\n",
    "        c_count = (y_train == lab).sum()\n",
    "        class_weights[lab] = 1.0 / (c_count + 1e-5)\n",
    "\n",
    "    distance = distances.CosineSimilarity()\n",
    "    loss_func = WeightedArcFaceLoss(\n",
    "        num_classes=num_classes,\n",
    "        embedding_size=128,\n",
    "        margin=28.6,\n",
    "        scale=64,\n",
    "        class_weights=class_weights,\n",
    "        distance=distance\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        list(model.parameters()) + list(loss_func.parameters()), lr=lr\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ep_start = time.time()\n",
    "        avg_loss = train_one_epoch(model, loss_func, device, loader, optimizer)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"  [Epoch {epoch}] train_loss={avg_loss:.4f}, time={(time.time() - ep_start):.2f} sec\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings_128(model, X_input, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    学習済みモデルで128次元埋め込みを得る\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    model.eval()\n",
    "    dataset = TensorDataset(torch.tensor(X_input, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            emb = model(batch)\n",
    "            embs.append(emb.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "\n",
    "def compute_average_tracklet_similarity(gall_emb, label_embs):\n",
    "    \"\"\"\n",
    "    トラックレット vs ラベル埋め込み\n",
    "    shape (N_label, N_gall) のcos類似度行列を作り、全要素を平均\n",
    "    \"\"\"\n",
    "    sim_matrix = cosine_similarity(label_embs, gall_emb)\n",
    "    avg_sim = np.mean(sim_matrix)\n",
    "    return avg_sim\n",
    "\n",
    "\n",
    "def predict_label_for_gall_tracklet(gall_emb, label_embs_dict):\n",
    "    \"\"\"\n",
    "    (best_label, best_score) を返す\n",
    "    \"\"\"\n",
    "    best_label = 0\n",
    "    best_score = -1e9\n",
    "    for lab, lab_embs in label_embs_dict.items():\n",
    "        avg_sim = compute_average_tracklet_similarity(gall_emb, lab_embs)\n",
    "        if avg_sim > best_score:\n",
    "            best_score = avg_sim\n",
    "            best_label = lab\n",
    "    return best_label, best_score\n",
    "\n",
    "\n",
    "def build_label_embs_dict(X_train, y_train, model, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    学習済みモデルに通して得た埋め込みをラベルごとにまとめる\n",
    "    \"\"\"\n",
    "    emb_train = get_embeddings_128(model, X_train, device=device, batch_size=batch_size)\n",
    "    label_map = defaultdict(list)\n",
    "    for emb_vec, lab in zip(emb_train, y_train):\n",
    "        label_map[lab].append(emb_vec)\n",
    "    label_embs_dict = {}\n",
    "    for lab, vectors in label_map.items():\n",
    "        label_embs_dict[lab] = np.stack(vectors, axis=0)\n",
    "    return label_embs_dict\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_random_seed(999)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ===============================\n",
    "    # 1) トレーニングデータ読み込み\n",
    "    # ===============================\n",
    "    print(\"\\n=== Loading train data (original only) from multiple pickle files ===\")\n",
    "    train_chunks_folder = \"C:/Users/sugie/PycharmProjects/pythonProject10/MARS/all_mini_direct/train_split_ReSNet_pkl/filtered\"\n",
    "    train_chunk_files = sorted([os.path.join(train_chunks_folder, f)\n",
    "                                for f in os.listdir(train_chunks_folder)\n",
    "                                if f.lower().endswith(\".pkl\")])\n",
    "    print(f\"Found {len(train_chunk_files)} train chunk PKL files in '{train_chunks_folder}'\")\n",
    "\n",
    "    total_train_samples = 70000\n",
    "    num_files = len(train_chunk_files)\n",
    "    samples_per_file = total_train_samples // num_files\n",
    "    remainder = total_train_samples % num_files\n",
    "\n",
    "    train_dfs = []\n",
    "    for i, file in enumerate(train_chunk_files):\n",
    "        df_part = pd.read_pickle(file)\n",
    "        if isinstance(df_part, list):\n",
    "            df_part = pd.DataFrame(df_part)\n",
    "        n_samples = samples_per_file + (1 if i < remainder else 0)\n",
    "        df_sampled = df_part.sample(n=n_samples, random_state=42)\n",
    "        train_dfs.append(df_sampled)\n",
    "        del df_part, df_sampled\n",
    "        gc.collect()\n",
    "    df_train_orig = pd.concat(train_dfs, ignore_index=True)\n",
    "    print(f\"Combined train data shape={df_train_orig.shape}\")\n",
    "    del train_dfs\n",
    "    gc.collect()\n",
    "\n",
    "    # ===============================\n",
    "    # 2) クエリ / ギャラリーの初期設定\n",
    "    # ===============================\n",
    "    print(\"\\n=== Building pid-to-file mapping for query data ===\")\n",
    "    query_chunks_folder = \"C:/Users/sugie/PycharmProjects/pythonProject10/MARS/all_mini_direct/query_split_ReSNet_pkl_mini/fillter\"\n",
    "    query_chunk_files = sorted([os.path.join(query_chunks_folder, f)\n",
    "                                for f in os.listdir(query_chunks_folder)\n",
    "                                if f.lower().endswith(\".pkl\")])\n",
    "    print(f\"Found {len(query_chunk_files)} query chunk PKL files in '{query_chunks_folder}'\")\n",
    "\n",
    "    pid_to_file = {}\n",
    "    for qf in query_chunk_files:\n",
    "        df_qpart = pd.read_pickle(qf)\n",
    "        if isinstance(df_qpart, list):\n",
    "            df_qpart = pd.DataFrame(df_qpart)\n",
    "        for pid_ in df_qpart[\"pid\"].unique():\n",
    "            pid_to_file[pid_] = qf\n",
    "        del df_qpart\n",
    "        gc.collect()\n",
    "\n",
    "    unique_query_pids = sorted(pid_to_file.keys())\n",
    "    chunk_size_q = 7\n",
    "    num_chunks = (len(unique_query_pids) + chunk_size_q - 1) // chunk_size_q\n",
    "    print(f\"Total query pids = {len(unique_query_pids)} => {num_chunks} query chunks (size={chunk_size_q})\")\n",
    "\n",
    "    # ギャラリー\n",
    "    gallery_chunks_folder = \"C:/Users/sugie/PycharmProjects/pythonProject10/MARS/all_mini_direct/gallery_split_ReSNet_pkl_mini/fillter\"\n",
    "    gallery_chunk_files = sorted([os.path.join(gallery_chunks_folder, f)\n",
    "                                  for f in os.listdir(gallery_chunks_folder)\n",
    "                                  if f.lower().endswith(\".pkl\")])\n",
    "    print(f\"Found {len(gallery_chunk_files)} gallery chunk PKL files in '{gallery_chunks_folder}'\")\n",
    "\n",
    "    print(\"Resetting `annotated=False` in all gallery chunk PKL files...\")\n",
    "    for gal_fpath in gallery_chunk_files:\n",
    "        df_gal_chunk = pd.read_pickle(gal_fpath)\n",
    "        if isinstance(df_gal_chunk, list):\n",
    "            df_gal_chunk = pd.DataFrame(df_gal_chunk)\n",
    "        df_gal_chunk[\"annotated\"] = False\n",
    "        df_gal_chunk.to_pickle(gal_fpath)\n",
    "        del df_gal_chunk\n",
    "        gc.collect()\n",
    "    print(\"All gallery chunk PKLs have annotated=False initialized.\")\n",
    "\n",
    "    # もし古いサマリーがあれば削除\n",
    "    summary_csv = \"summary_results_aug_matrix_next.csv\"\n",
    "    if os.path.exists(summary_csv):\n",
    "        os.remove(summary_csv)\n",
    "\n",
    "    # ===============================\n",
    "    # 3) Queryを7個ずつ追加しながら学習＆アノテーションフロー\n",
    "    # ===============================\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        chunk_start_t = time.time()\n",
    "        start_idx = chunk_idx * chunk_size_q\n",
    "        end_idx = min((chunk_idx+1)*chunk_size_q, len(unique_query_pids))\n",
    "        pids_to_add = unique_query_pids[start_idx:end_idx]\n",
    "        print(f\"\\n=== Query Chunk {chunk_idx+1}/{num_chunks} ===\")\n",
    "        print(f\"  -> pids_to_add = {pids_to_add}\")\n",
    "\n",
    "        print(\"Resetting annotated=False for all gallery chunk PKLs (start of THIS Query Chunk)...\")\n",
    "        for gal_fpath in gallery_chunk_files:\n",
    "            df_gal_chunk = pd.read_pickle(gal_fpath)\n",
    "            # \"annotated\"列が無ければ追加\n",
    "            if \"annotated\" not in df_gal_chunk.columns:\n",
    "                df_gal_chunk[\"annotated\"] = False\n",
    "            else:\n",
    "                df_gal_chunk[\"annotated\"] = False\n",
    "            df_gal_chunk.to_pickle(gal_fpath)\n",
    "            del df_gal_chunk\n",
    "            gc.collect()\n",
    "\n",
    "        # (A) クエリ取り出し\n",
    "        needed_filepaths = set(pid_to_file[pid_] for pid_ in pids_to_add)\n",
    "        query_dfs_for_chunk = []\n",
    "        for fpath in needed_filepaths:\n",
    "            df_qpart = pd.read_pickle(fpath)\n",
    "            if isinstance(df_qpart, list):\n",
    "                df_qpart = pd.DataFrame(df_qpart)\n",
    "            df_qpart_sub = df_qpart[df_qpart[\"pid\"].isin(pids_to_add)]\n",
    "            if len(df_qpart_sub) > 0:\n",
    "                query_dfs_for_chunk.append(df_qpart_sub)\n",
    "            del df_qpart, df_qpart_sub\n",
    "            gc.collect()\n",
    "\n",
    "        if len(query_dfs_for_chunk) == 0:\n",
    "            print(\"  -> No tracklets found for these PIDs in splitted query files.\")\n",
    "            continue\n",
    "\n",
    "        df_query_chunk = pd.concat(query_dfs_for_chunk, ignore_index=True)\n",
    "        del query_dfs_for_chunk\n",
    "        gc.collect()\n",
    "\n",
    "        # 「各PIDについて1つ目のトラックレット」を抜き出す\n",
    "        selected_tracklets = []\n",
    "        for pid_ in pids_to_add:\n",
    "            df_pid = df_query_chunk[df_query_chunk[\"pid\"] == pid_]\n",
    "            if df_pid.empty:\n",
    "                continue\n",
    "            tracklet_id = df_pid[\"tracklet_id\"].unique()[0]\n",
    "            df_tracklet = df_pid[df_pid[\"tracklet_id\"] == tracklet_id]\n",
    "            selected_tracklets.append(df_tracklet)\n",
    "            del df_pid, df_tracklet\n",
    "            gc.collect()\n",
    "\n",
    "        if len(selected_tracklets) == 0:\n",
    "            print(\"  -> No valid tracklets found for these PIDs.\")\n",
    "            continue\n",
    "\n",
    "        df_initial_add = pd.concat(selected_tracklets, ignore_index=True)\n",
    "        del selected_tracklets\n",
    "        gc.collect()\n",
    "\n",
    "        count_orig = (df_initial_add[\"is_aug\"] == 0).sum()\n",
    "        count_aug = (df_initial_add[\"is_aug\"] == 1).sum()\n",
    "        print(f\"  -> Add {len(df_initial_add)} query samples to training (original={count_orig}, augmentation={count_aug})\")\n",
    "\n",
    "        # new_train\n",
    "        new_train = pd.concat([df_train_orig.copy(), df_initial_add], ignore_index=True)\n",
    "        del df_initial_add\n",
    "        gc.collect()\n",
    "\n",
    "        # ラウンドを複数回まわす\n",
    "        for round_idx in range(1, 4):\n",
    "            round_start_t = time.time()\n",
    "            print(f\"\\n--- Query Chunk {chunk_idx+1}, Round {round_idx} ---\")\n",
    "\n",
    "            # (1) map_pids_to_0_known + downsample\n",
    "            df_train_mapped = new_train.copy()\n",
    "            df_train_mapped = map_pids_to_0_known(df_train_mapped, pids_to_add,\n",
    "                                                  label_col=\"pid\", new_label_col=\"pid_trans\")\n",
    "            df_train_mapped = downsample_label0(df_train_mapped, label_col=\"pid_trans\")\n",
    "\n",
    "            # (2) 学習\n",
    "            X_train_m, y_train_m = extract_features_and_labels(df_train_mapped)\n",
    "            model = train_net128_weighted_arcface(\n",
    "                X_train_m, y_train_m, device=device,\n",
    "                epochs=100, lr=1e-6, batch_size=128\n",
    "            )\n",
    "            del X_train_m, y_train_m\n",
    "            gc.collect()\n",
    "\n",
    "            # (3) ラベルごとの埋め込み辞書\n",
    "            X_label, y_label = extract_features_and_labels(df_train_mapped)\n",
    "            label_embs_dict = build_label_embs_dict(X_label, y_label, model, device=device)\n",
    "            del df_train_mapped, X_label, y_label\n",
    "            gc.collect()\n",
    "\n",
    "            # (4) ギャラリー推論\n",
    "            all_results = []\n",
    "            for gal_fpath in gallery_chunk_files:\n",
    "                df_gal_chunk = pd.read_pickle(gal_fpath)\n",
    "\n",
    "                df_gal_chunk = map_pids_to_0_known(df_gal_chunk, pids_to_add,\n",
    "                                                   label_col=\"pid\", new_label_col=\"pid_trans\")\n",
    "\n",
    "                df_gal_infer = df_gal_chunk[df_gal_chunk[\"is_aug\"]==0]\n",
    "                if len(df_gal_infer)==0:\n",
    "                    del df_gal_chunk, df_gal_infer\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "\n",
    "                track_ids = df_gal_infer[\"tracklet_id\"].unique()\n",
    "                pred_list = []\n",
    "                for tid in track_ids:\n",
    "                    sub_t = df_gal_infer[df_gal_infer[\"tracklet_id\"]==tid]\n",
    "                    X_gall_t, _ = extract_features_and_labels(sub_t)\n",
    "                    emb_gall_t = get_embeddings_128(model, X_gall_t, device=device)\n",
    "                    pid_true = sub_t[\"pid_trans\"].iloc[0]\n",
    "\n",
    "                    pred_label, best_score = predict_label_for_gall_tracklet(emb_gall_t, label_embs_dict)\n",
    "                    pred_list.append((tid, pid_true, pred_label, best_score, gal_fpath))\n",
    "\n",
    "                df_res_chunk = pd.DataFrame(pred_list, columns=[\"track_id\",\"pid_true\",\"pid_pred\",\"score\",\"gal_fpath\"])\n",
    "\n",
    "                # annotated情報をマージ\n",
    "                df_anno_chunk = df_gal_chunk[[\"tracklet_id\",\"annotated\"]].drop_duplicates()\n",
    "                df_anno_chunk[\"gal_fpath\"] = gal_fpath\n",
    "\n",
    "                df_res_chunk = pd.merge(\n",
    "                    df_res_chunk,\n",
    "                    df_anno_chunk,\n",
    "                    how=\"left\",\n",
    "                    left_on=[\"track_id\",\"gal_fpath\"],\n",
    "                    right_on=[\"tracklet_id\",\"gal_fpath\"]\n",
    "                )\n",
    "                df_res_chunk.drop(columns=[\"tracklet_id\"], inplace=True)\n",
    "\n",
    "                all_results.append(df_res_chunk)\n",
    "                del df_gal_chunk, df_gal_infer, df_res_chunk, df_anno_chunk\n",
    "                gc.collect()\n",
    "\n",
    "            if not all_results:\n",
    "                print(\"No tracklets were inferred in this round.\")\n",
    "                continue\n",
    "\n",
    "            df_res_all = pd.concat(all_results, ignore_index=True)\n",
    "            df_res_all[\"correct\"] = (df_res_all[\"pid_true\"] == df_res_all[\"pid_pred\"]).astype(int)\n",
    "\n",
    "            # (5) レポート\n",
    "            y_true_mv = df_res_all[\"pid_true\"].values\n",
    "            y_pred_mv = df_res_all[\"pid_pred\"].values\n",
    "            rep_dict = classification_report(y_true_mv, y_pred_mv, output_dict=True, zero_division=0)\n",
    "            rep_df = pd.DataFrame(rep_dict).transpose().round(4)\n",
    "            print(\"\\n=== classification_report (tracklet-level) ===\")\n",
    "            print(rep_df)\n",
    "\n",
    "            grp = df_res_all.groupby(\"pid_true\")[\"correct\"].agg(total=\"count\", correct=\"sum\")\n",
    "            grp[\"detect_rate(%)\"] = 100.0 * grp[\"correct\"]/grp[\"total\"]\n",
    "            print(\"\\n=== Per-PID detection in this round (ALL) ===\")\n",
    "            print(grp.sort_values(\"pid_true\").head(30))\n",
    "\n",
    "            # (7) 「annotated=False かつ pid_pred!=0」からアノテーション\n",
    "            df_valid = df_res_all[\n",
    "                (df_res_all[\"pid_pred\"] != 0) &\n",
    "                (df_res_all[\"annotated\"] == False)\n",
    "            ].copy()\n",
    "            # スコア降順に並べる\n",
    "            df_valid.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "\n",
    "            # まずは「pred_label(=pid_pred)ごとに上位5件」をピックアップ\n",
    "            label_top5_ids = []\n",
    "            for pred_lab, sub_df in df_valid.groupby(\"pid_pred\"):\n",
    "                top_5_tracks = sub_df.head(5)[\"track_id\"].tolist()\n",
    "                label_top5_ids.extend(top_5_tracks)\n",
    "\n",
    "            # 重複除去(一応)\n",
    "            label_top5_ids = list(dict.fromkeys(label_top5_ids))\n",
    "\n",
    "            # トータルでアノテーションしたい最大数(例: 50)\n",
    "            annotation_capacity = 50\n",
    "\n",
    "            # 既に「ラベルごと上位5件」でこれだけ\n",
    "            n_label_based = len(label_top5_ids)\n",
    "            if n_label_based >= annotation_capacity:\n",
    "                # もし label_top5_ids が 50を超える場合は、その中でscoreが高い順に 50個 だけ採用\n",
    "                label_top5_df = df_valid[df_valid[\"track_id\"].isin(label_top5_ids)].copy()\n",
    "                label_top5_df.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "                final_ids_to_annotate = label_top5_df[\"track_id\"].head(annotation_capacity).tolist()\n",
    "            else:\n",
    "                # 残り枠\n",
    "                remain_needed = annotation_capacity - n_label_based\n",
    "                # 「ラベル別上位5」に含まれなかった残り\n",
    "                df_leftover = df_valid[~df_valid[\"track_id\"].isin(label_top5_ids)].copy()\n",
    "                # そこからスコア上位 'remain_needed' 件をピックアップ\n",
    "                leftover_ids = df_leftover[\"track_id\"].head(remain_needed).tolist()\n",
    "                final_ids_to_annotate = label_top5_ids + leftover_ids\n",
    "\n",
    "            print(f\"  -> We'll annotate up to {annotation_capacity} tracklets.\")\n",
    "            print(f\"  -> label-based picks = {n_label_based}, final to annotate = {len(final_ids_to_annotate)}\")\n",
    "\n",
    "            # (8) アノテーション処理\n",
    "            newly_annotated_count = 0\n",
    "            cond_correct_list = []\n",
    "\n",
    "            df_topN = df_valid[df_valid[\"track_id\"].isin(final_ids_to_annotate)].copy()\n",
    "            group_fpath = df_topN.groupby(\"gal_fpath\")[\"track_id\"].unique()\n",
    "\n",
    "            for gfpath, tids_array in group_fpath.items():\n",
    "                df_gal_chunk = pd.read_pickle(gfpath)\n",
    "                df_gal_chunk = map_pids_to_0_known(df_gal_chunk, pids_to_add,\n",
    "                                                   label_col=\"pid\", new_label_col=\"pid_trans\")\n",
    "\n",
    "                df_to_add = df_gal_chunk[\n",
    "                    (df_gal_chunk[\"tracklet_id\"].isin(tids_array)) &\n",
    "                    (df_gal_chunk[\"annotated\"] == False)\n",
    "                ].copy()\n",
    "                if not df_to_add.empty:\n",
    "                    newly_ids = df_to_add[\"tracklet_id\"].unique()\n",
    "                    newly_annotated_count += len(newly_ids)\n",
    "\n",
    "                    # 正解行を抽出\n",
    "                    cond_correct_part = df_res_all[\n",
    "                        (df_res_all[\"track_id\"].isin(newly_ids)) &\n",
    "                        (df_res_all[\"correct\"] == 1)\n",
    "                    ]\n",
    "                    if not cond_correct_part.empty:\n",
    "                        cond_correct_list.append(cond_correct_part)\n",
    "\n",
    "                    # tracklet_id の振り直し\n",
    "                    max_tid = new_train[\"tracklet_id\"].max() if \"tracklet_id\" in new_train.columns else -1\n",
    "                    df_to_add[\"tracklet_id\"] = df_to_add[\"tracklet_id\"] + max_tid + 1\n",
    "\n",
    "                    # new_trainに加える\n",
    "                    new_train = pd.concat([new_train, df_to_add], ignore_index=True)\n",
    "\n",
    "                    # annotated=True に更新\n",
    "                    original_idx = df_gal_chunk[\n",
    "                        (df_gal_chunk[\"tracklet_id\"].isin(newly_ids)) &\n",
    "                        (df_gal_chunk[\"annotated\"] == False)\n",
    "                    ].index\n",
    "                    df_gal_chunk.loc[original_idx, \"annotated\"] = True\n",
    "\n",
    "                    # 書き戻し\n",
    "                    df_gal_chunk.to_pickle(gfpath)\n",
    "\n",
    "                del df_gal_chunk, df_to_add\n",
    "                gc.collect()\n",
    "\n",
    "            # まとめて正解数をカウント\n",
    "            if cond_correct_list:\n",
    "                cond_correct_all = pd.concat(cond_correct_list, ignore_index=True)\n",
    "                correct_newly_annotated = cond_correct_all.shape[0]\n",
    "            else:\n",
    "                cond_correct_all = pd.DataFrame()\n",
    "                correct_newly_annotated = 0\n",
    "\n",
    "            print(f\"  -> newly_annotated={newly_annotated_count}, correct_newly_annotated={correct_newly_annotated}\")\n",
    "\n",
    "            if not cond_correct_all.empty:\n",
    "                grp_label = cond_correct_all.groupby(\"pid_true\").size().reset_index(name=\"correct_count\")\n",
    "                print(\"\\n=== correct_newly_annotated breakdown by pid_true ===\")\n",
    "                print(grp_label.sort_values(\"pid_true\"))\n",
    "            else:\n",
    "                print(\"\\nNo newly annotated tracklets in this round -> No breakdown\")\n",
    "\n",
    "            # ★ 追加: 内訳を文字列化\n",
    "            if not cond_correct_all.empty:\n",
    "                grp_label = cond_correct_all.groupby(\"pid_true\").size().reset_index(name=\"correct_count\")\n",
    "                print(\"\\n=== correct_newly_annotated breakdown by pid_true ===\")\n",
    "                print(grp_label.sort_values(\"pid_true\"))\n",
    "\n",
    "                breakdown_str = \"; \".join(f\"{int(r.pid_true)}:{int(r.correct_count)}\"\n",
    "                                              for r in grp_label.itertuples())\n",
    "            else:\n",
    "                print(\"\\nNo newly annotated tracklets in this round -> No breakdown\")\n",
    "                breakdown_str = \"\"\n",
    "\n",
    "            # (9) ログ書き出し\n",
    "            row_dict = {\n",
    "                \"chunk\": chunk_idx+1,\n",
    "                \"round\": round_idx,\n",
    "                \"pids_to_add\": str(pids_to_add),\n",
    "                \"train_samples\": len(new_train),\n",
    "                \"newly_annotated_count\": newly_annotated_count,\n",
    "                \"correct_newly_annotated\": correct_newly_annotated,\n",
    "                \"correct_newly_annotated_breakdown\": breakdown_str\n",
    "            }\n",
    "            df_summary = pd.DataFrame([row_dict])\n",
    "            if not os.path.exists(summary_csv):\n",
    "                df_summary.to_csv(summary_csv, index=False)\n",
    "            else:\n",
    "                df_summary.to_csv(summary_csv, index=False, mode=\"a\", header=False)\n",
    "            del df_summary, row_dict\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"\\n[Chunk {chunk_idx+1} done] pids_to_add={pids_to_add}, chunk_time={(time.time()-chunk_start_t):.2f}s\")\n",
    "\n",
    "    print(\"\\n=== All Done ===\")\n",
    "    print(\"See 'summary_results_aug_matrix.csv' for chunk-by-chunk progress.\")\n",
    "    print(\"Also check the classification reports printed each round for details.\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10331a10-7d3c-44bf-b3f8-9a44ecc3214f",
   "metadata": {},
   "source": [
    "# 必要ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5405ab4e-7eff-429b-a716-c2636d99ff6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_metric_learning import distances\n",
    "from pytorch_metric_learning.losses import ArcFaceLoss\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b8354-94a1-4068-bfe3-21a8221bfe27",
   "metadata": {},
   "source": [
    "# 学習やデータ前処理に関するユーティリティ関数群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cae367d-dc75-4aff-98d1-2b5368dacbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_pids_to_0_known(df, known_pids, label_col=\"pid\", new_label_col=\"pid_trans\"):\n",
    "    \"\"\"\n",
    "    既知の pid (known_pids) に含まれるIDは 1,2,3,…\n",
    "    それ以外は 0 としてマッピングする。\n",
    "    \n",
    "    例:\n",
    "    known_pids = [10, 11, 15] => これらを [1, 2, 3] に割り当て\n",
    "    その他のpid => 0\n",
    "    \"\"\"\n",
    "    sorted_kps = sorted(known_pids)\n",
    "    pid_map = {p: i + 1 for i, p in enumerate(sorted_kps)}\n",
    "    df[new_label_col] = df[label_col].apply(lambda x: pid_map.get(x, 0))\n",
    "    return df\n",
    "\n",
    "\n",
    "def downsample_label0(df, label_col=\"pid\"):\n",
    "    \"\"\"\n",
    "    未知(=0)ラベルのサンプルが多すぎる場合にダウンサンプリングを行う。\n",
    "    データ不均衡を緩和して学習を安定させるのが目的。\n",
    "    \"\"\"\n",
    "    label0_df = df[df[label_col] == 0]\n",
    "    others_df = df[df[label_col] != 0]\n",
    "    counts = others_df[label_col].value_counts()\n",
    "    max_count = counts.max() if not counts.empty else 0\n",
    "    # 他クラスの最大数 * 30 か最低128を基準に制限\n",
    "    max_count = max(max_count * 30, 128)\n",
    "    print(\"各ラベルのサンプル数:\")\n",
    "    print(df[label_col].value_counts())\n",
    "    print(f\"→ ラベル0を {max_count} 枚に合わせます\")\n",
    "    if len(label0_df) > max_count:\n",
    "        label0_df = label0_df.sample(n=max_count, random_state=42)\n",
    "    new_df = pd.concat([label0_df, others_df], ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def set_random_seed(seed_value=42):\n",
    "    \"\"\"\n",
    "    乱数シードを固定して学習の再現性をある程度確保する。\n",
    "    \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def extract_features_and_labels(df):\n",
    "    \"\"\"\n",
    "    DataFrameから特徴量Xとラベルyを抽出して返す。\n",
    "    - embedding列がある場合: (N, dim) にスタック\n",
    "    - embedding列がない場合: dim0, dim1, ... という列をまとめて (N, dim) に\n",
    "    - ラベルは pid_trans が優先、それが無ければ pid 列\n",
    "    \"\"\"\n",
    "    if \"embedding\" in df.columns:\n",
    "        X = np.stack(df[\"embedding\"].values)\n",
    "    else:\n",
    "        feature_cols = [c for c in df.columns if c.startswith(\"dim\")]\n",
    "        X = df[feature_cols].values\n",
    "\n",
    "    if \"pid_trans\" in df.columns:\n",
    "        y = df[\"pid_trans\"].values\n",
    "    else:\n",
    "        y = df[\"pid\"].values\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e294ad9e-5207-462c-ab77-bf10bb117732",
   "metadata": {},
   "source": [
    "# WeightedArcFaceLoss クラス定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bc70c1-a3c9-44fc-a66d-707d78eb4f4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ArcFaceLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWeightedArcFaceLoss\u001b[39;00m(\u001b[43mArcFaceLoss\u001b[49m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    ArcFaceLossを継承し、クラスごとに異なる重み付けを行うためのCrossEntropyLossを組み合わせたクラス。\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      7\u001b[0m             num_classes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     13\u001b[0m     ):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ArcFaceLoss' is not defined"
     ]
    }
   ],
   "source": [
    "class WeightedArcFaceLoss(ArcFaceLoss):\n",
    "    \"\"\"\n",
    "    ArcFaceLossを継承し、クラスごとに異なる重み付けを行うためのCrossEntropyLossを組み合わせたクラス。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes,\n",
    "            embedding_size,\n",
    "            margin=28.6,\n",
    "            scale=64,\n",
    "            class_weights=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_classes=num_classes,\n",
    "            embedding_size=embedding_size,\n",
    "            margin=margin,\n",
    "            scale=scale,\n",
    "            **kwargs\n",
    "        )\n",
    "        if class_weights is not None:\n",
    "            if not isinstance(class_weights, torch.Tensor):\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        # reduction=\"none\" にすることで、後段でlossの平均を手動でとる操作が可能\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(\n",
    "            weight=class_weights,\n",
    "            reduction=\"none\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d554e-4543-4bd7-bc28-7e44b605f2cc",
   "metadata": {},
   "source": [
    "# ネットワークモデル Net128 の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a827e62-6304-4e98-8649-d3c33c8f6d4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet128\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    2048次元の入力（ResNetなどで得られた特徴量を想定）を\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    128次元まで圧縮するネットワーク。\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    DropoutやBatchNormにより過学習を抑制しつつ学習を行う。\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Net128(nn.Module):\n",
    "    \"\"\"\n",
    "    2048次元の入力（ResNetなどで得られた特徴量を想定）を\n",
    "    128次元まで圧縮するネットワーク。\n",
    "    DropoutやBatchNormにより過学習を抑制しつつ学習を行う。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net128, self).__init__()\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(512, 384)\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(384, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1aba6-d212-43bf-8397-10720dbeca18",
   "metadata": {},
   "source": [
    "# 学習（train）に関する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b60c51b-20c3-468b-949c-bcc04fe97fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_func, device, loader, optimizer):\n",
    "    \"\"\"\n",
    "    1エポック分の学習を行う。\n",
    "    Dataloaderからミニバッチを取り出し、順伝搬 -> 損失計算 -> 逆伝搬 -> パラメータ更新。\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for features, labels in loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(features)\n",
    "        loss_output = loss_func(embeddings, labels)\n",
    "        # WeightedArcFaceLossの場合、戻り値が辞書かどうか確認する\n",
    "        if isinstance(loss_output, dict):\n",
    "            loss_val = loss_output[\"loss\"][\"losses\"].mean()\n",
    "        else:\n",
    "            loss_val = loss_output\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss_val.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def train_net128_weighted_arcface(X_train, y_train, device, epochs=100, lr=1e-6, batch_size=128):\n",
    "    \"\"\"\n",
    "    ArcFaceを用いた学習を実施する。\n",
    "    - WeightedArcFaceLoss (クラス別重み付き)\n",
    "    - 学習率はデフォルトで 1e-6\n",
    "    - エポック数はデフォルトで 100\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    unique_labels = np.unique(y_train)\n",
    "    num_classes = unique_labels.max() + 1\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    model = Net128().to(device)\n",
    "\n",
    "    # クラス重みの計算（単純に 1/class_count）\n",
    "    class_weights = np.zeros(num_classes, dtype=np.float32)\n",
    "    for lab in unique_labels:\n",
    "        c_count = (y_train == lab).sum()\n",
    "        class_weights[lab] = 1.0 / (c_count + 1e-5)\n",
    "\n",
    "    distance = distances.CosineSimilarity()\n",
    "    loss_func = WeightedArcFaceLoss(\n",
    "        num_classes=num_classes,\n",
    "        embedding_size=128,\n",
    "        margin=28.6,\n",
    "        scale=64,\n",
    "        class_weights=class_weights,\n",
    "        distance=distance\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        list(model.parameters()) + list(loss_func.parameters()), lr=lr\n",
    "    )\n",
    "    # 損失が改善しない場合に学習率を下げるスケジューラ\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ep_start = time.time()\n",
    "        avg_loss = train_one_epoch(model, loss_func, device, loader, optimizer)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"  [Epoch {epoch}] train_loss={avg_loss:.4f}, time={(time.time() - ep_start):.2f} sec\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1baf88c-d349-4045-b967-61538dc548ab",
   "metadata": {},
   "source": [
    "# 推論に関する関数 (埋め込み取得、類似度計算など)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b34ebb-d47b-4b73-844e-513438dc6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_128(model, X_input, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    学習済みモデルを使って 128次元埋め込みを取得する。\n",
    "    入力 X_input: shape = (N, 2048) を想定\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    model.eval()\n",
    "    dataset = TensorDataset(torch.tensor(X_input, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            emb = model(batch)\n",
    "            embs.append(emb.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "\n",
    "def compute_average_tracklet_similarity(gall_emb, label_embs):\n",
    "    \"\"\"\n",
    "    与えられたギャラリー埋め込み（gall_emb）と、あるラベルの埋め込み集合（label_embs）の\n",
    "    コサイン類似度を計算し、その平均を返す。\n",
    "    \"\"\"\n",
    "    sim_matrix = cosine_similarity(label_embs, gall_emb)\n",
    "    avg_sim = np.mean(sim_matrix)\n",
    "    return avg_sim\n",
    "\n",
    "\n",
    "def predict_label_for_gall_tracklet(gall_emb, label_embs_dict):\n",
    "    \"\"\"\n",
    "    ギャラリーの一つのトラックレット埋め込み gall_emb が\n",
    "    どのラベルに最も近いかを類似度ベースで判定する。\n",
    "    戻り値: (best_label, best_score)\n",
    "    \"\"\"\n",
    "    best_label = 0\n",
    "    best_score = -1e9\n",
    "    for lab, lab_embs in label_embs_dict.items():\n",
    "        avg_sim = compute_average_tracklet_similarity(gall_emb, lab_embs)\n",
    "        if avg_sim > best_score:\n",
    "            best_score = avg_sim\n",
    "            best_label = lab\n",
    "    return best_label, best_score\n",
    "\n",
    "\n",
    "def build_label_embs_dict(X_train, y_train, model, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    学習済みモデルを用いて X_train -> 128次元埋め込みを取得し、\n",
    "    ラベルごとに埋め込みをまとめた辞書を作成する。\n",
    "    label_embs_dict[label] = (N_label_samples, 128)\n",
    "    \"\"\"\n",
    "    emb_train = get_embeddings_128(model, X_train, device=device, batch_size=batch_size)\n",
    "    label_map = defaultdict(list)\n",
    "    for emb_vec, lab in zip(emb_train, y_train):\n",
    "        label_map[lab].append(emb_vec)\n",
    "    label_embs_dict = {}\n",
    "    for lab, vectors in label_map.items():\n",
    "        label_embs_dict[lab] = np.stack(vectors, axis=0)\n",
    "    return label_embs_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a494e87-ad48-4d3e-b768-8915d72bfe2b",
   "metadata": {},
   "source": [
    "# 関数実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97196dd7-f45c-41e6-8aba-62a1ceaa6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Cell 7: メインフロー main() 関数の定義\n",
    "##############################################\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    データの読み込み、学習、推論、アノテーションフローのサイクルをまとめたメイン関数。\n",
    "    ピックアップされたクエリのトラックレットを既存学習データに追加し、\n",
    "    ArcFaceベースの学習・推論を繰り返しながらギャラリーのアノテーションを進める。\n",
    "    \"\"\"\n",
    "    set_random_seed(999)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ===============================\n",
    "    # 1) トレーニングデータ読み込み\n",
    "    # ===============================\n",
    "    print(\"\\n=== Loading train data (original only) from multiple pickle files ===\")\n",
    "    train_chunks_folder = \"C:/Users/sugie/PycharmProjects/pythonProject10/MARS/all_mini_direct/train_split_ReSNet_pkl/filtered\"\n",
    "    train_chunk_files = sorted([os.path.join(train_chunks_folder, f)\n",
    "                                for f in os.listdir(train_chunks_folder)\n",
    "                                if f.lower().endswith(\".pkl\")])\n",
    "    print(f\"Found {len(train_chunk_files)} train chunk PKL files in '{train_chunks_folder}'\")\n",
    "\n",
    "    total_train_samples = 70000\n",
    "    num_files = len(train_chunk_files)\n",
    "    samples_per_file = total_train_samples // num_files\n",
    "    remainder = total_train_samples % num_files\n",
    "\n",
    "    train_dfs = []\n",
    "    for i, file in enumerate(train_chunk_files):\n",
    "        df_part = pd.read_pickle(file)\n",
    "        if isinstance(df_part, list):\n",
    "            df_part = pd.DataFrame(df_part)\n",
    "        n_samples = samples_per_file + (1 if i < remainder else 0)\n",
    "        df_sampled = df_part.sample(n=n_samples, random_state=42)\n",
    "        train_dfs.append(df_sampled)\n",
    "        del df_part, df_sampled\n",
    "        gc.collect()\n",
    "    df_train_orig = pd.concat(train_dfs, ignore_index=True)\n",
    "    print(f\"Combined train data shape={df_train_orig.shape}\")\n",
    "    del train_dfs\n",
    "    gc.collect()\n",
    "\n",
    "    # ===============================\n",
    "    # 2) クエリ / ギャラリーの初期設定\n",
    "    # ===============================\n",
    "    print(\"\\n=== Building pid-to-file mapping for query data ===\")\n",
    "    query_chunks_folder = \"C:/Users/sugie/PycharmProjects/pythonProject10/MARS/all_mini_direct/query_split_ReSNet_pkl_mini/fillter\"\n",
    "    query_chunk_files = sorted([os.path.join(query_chunks_folder, f)\n",
    "                                for f in os.listdir(query_chunks_folder)\n",
    "                                if f.lower().endswith(\".pkl\")])\n",
    "    print(f\"Found {len(query_chunk_files)} query chunk PKL files in '{query_chunks_folder}'\")\n",
    "\n",
    "    pid_to_file = {}\n",
    "    for qf in query_chunk_files:\n",
    "        df_qpart = pd.read_pickle(qf)\n",
    "        if isinstance(df_qpart, list):\n",
    "            df_qpart = pd.DataFrame(df_qpart)\n",
    "        for pid_ in df_qpart[\"pid\"].unique():\n",
    "            pid_to_file[pid_] = qf\n",
    "        del df_qpart\n",
    "        gc.collect()\n",
    "\n",
    "    unique_query_pids = sorted(pid_to_file.keys())\n",
    "    chunk_size_q = 7\n",
    "    num_chunks = (len(unique_query_pids) + chunk_size_q - 1) // chunk_size_q\n",
    "    print(f\"Total query pids = {len(unique_query_pids)} => {num_chunks} query chunks (size={chunk_size_q})\")\n",
    "\n",
    "    # ギャラリー\n",
    "    gallery_chunks_folder = \"C:/Users/sugie/PycharmProjects/pythonProject10/MARS/all_mini_direct/gallery_split_ReSNet_pkl_mini/fillter\"\n",
    "    gallery_chunk_files = sorted([os.path.join(gallery_chunks_folder, f)\n",
    "                                  for f in os.listdir(gallery_chunks_folder)\n",
    "                                  if f.lower().endswith(\".pkl\")])\n",
    "    print(f\"Found {len(gallery_chunk_files)} gallery chunk PKL files in '{gallery_chunks_folder}'\")\n",
    "\n",
    "    print(\"Resetting `annotated=False` in all gallery chunk PKL files...\")\n",
    "    for gal_fpath in gallery_chunk_files:\n",
    "        df_gal_chunk = pd.read_pickle(gal_fpath)\n",
    "        if isinstance(df_gal_chunk, list):\n",
    "            df_gal_chunk = pd.DataFrame(df_gal_chunk)\n",
    "        df_gal_chunk[\"annotated\"] = False\n",
    "        df_gal_chunk.to_pickle(gal_fpath)\n",
    "        del df_gal_chunk\n",
    "        gc.collect()\n",
    "    print(\"All gallery chunk PKLs have annotated=False initialized.\")\n",
    "\n",
    "    summary_csv = \"summary_results_aug_matrix_next.csv\"\n",
    "    if os.path.exists(summary_csv):\n",
    "        os.remove(summary_csv)\n",
    "\n",
    "    # ===============================\n",
    "    # 3) Queryを7個ずつ追加しながら学習＆アノテーションフロー\n",
    "    # ===============================\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        chunk_start_t = time.time()\n",
    "        start_idx = chunk_idx * chunk_size_q\n",
    "        end_idx = min((chunk_idx+1)*chunk_size_q, len(unique_query_pids))\n",
    "        pids_to_add = unique_query_pids[start_idx:end_idx]\n",
    "        print(f\"\\n=== Query Chunk {chunk_idx+1}/{num_chunks} ===\")\n",
    "        print(f\"  -> pids_to_add = {pids_to_add}\")\n",
    "\n",
    "        print(\"Resetting annotated=False for all gallery chunk PKLs (start of THIS Query Chunk)...\")\n",
    "        for gal_fpath in gallery_chunk_files:\n",
    "            df_gal_chunk = pd.read_pickle(gal_fpath)\n",
    "            if \"annotated\" not in df_gal_chunk.columns:\n",
    "                df_gal_chunk[\"annotated\"] = False\n",
    "            else:\n",
    "                df_gal_chunk[\"annotated\"] = False\n",
    "            df_gal_chunk.to_pickle(gal_fpath)\n",
    "            del df_gal_chunk\n",
    "            gc.collect()\n",
    "\n",
    "        # クエリを取り出して結合\n",
    "        needed_filepaths = set(pid_to_file[pid_] for pid_ in pids_to_add)\n",
    "        query_dfs_for_chunk = []\n",
    "        for fpath in needed_filepaths:\n",
    "            df_qpart = pd.read_pickle(fpath)\n",
    "            if isinstance(df_qpart, list):\n",
    "                df_qpart = pd.DataFrame(df_qpart)\n",
    "            df_qpart_sub = df_qpart[df_qpart[\"pid\"].isin(pids_to_add)]\n",
    "            if len(df_qpart_sub) > 0:\n",
    "                query_dfs_for_chunk.append(df_qpart_sub)\n",
    "            del df_qpart, df_qpart_sub\n",
    "            gc.collect()\n",
    "\n",
    "        if len(query_dfs_for_chunk) == 0:\n",
    "            print(\"  -> No tracklets found for these PIDs in splitted query files.\")\n",
    "            continue\n",
    "\n",
    "        df_query_chunk = pd.concat(query_dfs_for_chunk, ignore_index=True)\n",
    "        del query_dfs_for_chunk\n",
    "        gc.collect()\n",
    "\n",
    "        # 最初のトラックレットだけを選択\n",
    "        selected_tracklets = []\n",
    "        for pid_ in pids_to_add:\n",
    "            df_pid = df_query_chunk[df_query_chunk[\"pid\"] == pid_]\n",
    "            if df_pid.empty:\n",
    "                continue\n",
    "            tracklet_id = df_pid[\"tracklet_id\"].unique()[0]\n",
    "            df_tracklet = df_pid[df_pid[\"tracklet_id\"] == tracklet_id]\n",
    "            selected_tracklets.append(df_tracklet)\n",
    "            del df_pid, df_tracklet\n",
    "            gc.collect()\n",
    "\n",
    "        if len(selected_tracklets) == 0:\n",
    "            print(\"  -> No valid tracklets found for these PIDs.\")\n",
    "            continue\n",
    "\n",
    "        df_initial_add = pd.concat(selected_tracklets, ignore_index=True)\n",
    "        del selected_tracklets\n",
    "        gc.collect()\n",
    "\n",
    "        count_orig = (df_initial_add[\"is_aug\"] == 0).sum()\n",
    "        count_aug = (df_initial_add[\"is_aug\"] == 1).sum()\n",
    "        print(f\"  -> Add {len(df_initial_add)} query samples to training (original={count_orig}, augmentation={count_aug})\")\n",
    "\n",
    "        new_train = pd.concat([df_train_orig.copy(), df_initial_add], ignore_index=True)\n",
    "        del df_initial_add\n",
    "        gc.collect()\n",
    "\n",
    "        # ラウンドを複数回まわす\n",
    "        for round_idx in range(1, 4):\n",
    "            round_start_t = time.time()\n",
    "            print(f\"\\n--- Query Chunk {chunk_idx+1}, Round {round_idx} ---\")\n",
    "\n",
    "            # (1) map_pids_to_0_known + downsample\n",
    "            df_train_mapped = new_train.copy()\n",
    "            df_train_mapped = map_pids_to_0_known(df_train_mapped, pids_to_add,\n",
    "                                                  label_col=\"pid\", new_label_col=\"pid_trans\")\n",
    "            df_train_mapped = downsample_label0(df_train_mapped, label_col=\"pid_trans\")\n",
    "\n",
    "            # (2) 学習\n",
    "            X_train_m, y_train_m = extract_features_and_labels(df_train_mapped)\n",
    "            model = train_net128_weighted_arcface(\n",
    "                X_train_m, y_train_m, device=device,\n",
    "                epochs=100, lr=1e-6, batch_size=128\n",
    "            )\n",
    "            del X_train_m, y_train_m\n",
    "            gc.collect()\n",
    "\n",
    "            # (3) ラベルごとの埋め込み辞書\n",
    "            X_label, y_label = extract_features_and_labels(df_train_mapped)\n",
    "            label_embs_dict = build_label_embs_dict(X_label, y_label, model, device=device)\n",
    "            del df_train_mapped, X_label, y_label\n",
    "            gc.collect()\n",
    "\n",
    "            # (4) ギャラリー推論\n",
    "            all_results = []\n",
    "            for gal_fpath in gallery_chunk_files:\n",
    "                df_gal_chunk = pd.read_pickle(gal_fpath)\n",
    "                df_gal_chunk = map_pids_to_0_known(df_gal_chunk, pids_to_add,\n",
    "                                                   label_col=\"pid\", new_label_col=\"pid_trans\")\n",
    "\n",
    "                df_gal_infer = df_gal_chunk[df_gal_chunk[\"is_aug\"]==0]\n",
    "                if len(df_gal_infer)==0:\n",
    "                    del df_gal_chunk, df_gal_infer\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "\n",
    "                track_ids = df_gal_infer[\"tracklet_id\"].unique()\n",
    "                pred_list = []\n",
    "                for tid in track_ids:\n",
    "                    sub_t = df_gal_infer[df_gal_infer[\"tracklet_id\"]==tid]\n",
    "                    X_gall_t, _ = extract_features_and_labels(sub_t)\n",
    "                    emb_gall_t = get_embeddings_128(model, X_gall_t, device=device)\n",
    "                    pid_true = sub_t[\"pid_trans\"].iloc[0]\n",
    "\n",
    "                    pred_label, best_score = predict_label_for_gall_tracklet(emb_gall_t, label_embs_dict)\n",
    "                    pred_list.append((tid, pid_true, pred_label, best_score, gal_fpath))\n",
    "\n",
    "                df_res_chunk = pd.DataFrame(pred_list, columns=[\"track_id\",\"pid_true\",\"pid_pred\",\"score\",\"gal_fpath\"])\n",
    "\n",
    "                # annotated情報をマージ\n",
    "                df_anno_chunk = df_gal_chunk[[\"tracklet_id\",\"annotated\"]].drop_duplicates()\n",
    "                df_anno_chunk[\"gal_fpath\"] = gal_fpath\n",
    "\n",
    "                df_res_chunk = pd.merge(\n",
    "                    df_res_chunk,\n",
    "                    df_anno_chunk,\n",
    "                    how=\"left\",\n",
    "                    left_on=[\"track_id\",\"gal_fpath\"],\n",
    "                    right_on=[\"tracklet_id\",\"gal_fpath\"]\n",
    "                )\n",
    "                df_res_chunk.drop(columns=[\"tracklet_id\"], inplace=True)\n",
    "\n",
    "                all_results.append(df_res_chunk)\n",
    "                del df_gal_chunk, df_gal_infer, df_res_chunk, df_anno_chunk\n",
    "                gc.collect()\n",
    "\n",
    "            if not all_results:\n",
    "                print(\"No tracklets were inferred in this round.\")\n",
    "                continue\n",
    "\n",
    "            df_res_all = pd.concat(all_results, ignore_index=True)\n",
    "            df_res_all[\"correct\"] = (df_res_all[\"pid_true\"] == df_res_all[\"pid_pred\"]).astype(int)\n",
    "\n",
    "            # (5) レポート\n",
    "            y_true_mv = df_res_all[\"pid_true\"].values\n",
    "            y_pred_mv = df_res_all[\"pid_pred\"].values\n",
    "            rep_dict = classification_report(y_true_mv, y_pred_mv, output_dict=True, zero_division=0)\n",
    "            rep_df = pd.DataFrame(rep_dict).transpose().round(4)\n",
    "            print(\"\\n=== classification_report (tracklet-level) ===\")\n",
    "            print(rep_df)\n",
    "\n",
    "            grp = df_res_all.groupby(\"pid_true\")[\"correct\"].agg(total=\"count\", correct=\"sum\")\n",
    "            grp[\"detect_rate(%)\"] = 100.0 * grp[\"correct\"]/grp[\"total\"]\n",
    "            print(\"\\n=== Per-PID detection in this round (ALL) ===\")\n",
    "            print(grp.sort_values(\"pid_true\").head(30))\n",
    "\n",
    "            # (7) 「annotated=False かつ pid_pred!=0」からアノテーション\n",
    "            df_valid = df_res_all[\n",
    "                (df_res_all[\"pid_pred\"] != 0) &\n",
    "                (df_res_all[\"annotated\"] == False)\n",
    "            ].copy()\n",
    "            df_valid.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "\n",
    "            # ラベルごと上位5件\n",
    "            label_top5_ids = []\n",
    "            for pred_lab, sub_df in df_valid.groupby(\"pid_pred\"):\n",
    "                top_5_tracks = sub_df.head(5)[\"track_id\"].tolist()\n",
    "                label_top5_ids.extend(top_5_tracks)\n",
    "\n",
    "            label_top5_ids = list(dict.fromkeys(label_top5_ids))  # 重複除去\n",
    "\n",
    "            annotation_capacity = 50\n",
    "            n_label_based = len(label_top5_ids)\n",
    "            if n_label_based >= annotation_capacity:\n",
    "                label_top5_df = df_valid[df_valid[\"track_id\"].isin(label_top5_ids)].copy()\n",
    "                label_top5_df.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "                final_ids_to_annotate = label_top5_df[\"track_id\"].head(annotation_capacity).tolist()\n",
    "            else:\n",
    "                remain_needed = annotation_capacity - n_label_based\n",
    "                df_leftover = df_valid[~df_valid[\"track_id\"].isin(label_top5_ids)].copy()\n",
    "                leftover_ids = df_leftover[\"track_id\"].head(remain_needed).tolist()\n",
    "                final_ids_to_annotate = label_top5_ids + leftover_ids\n",
    "\n",
    "            print(f\"  -> We'll annotate up to {annotation_capacity} tracklets.\")\n",
    "            print(f\"  -> label-based picks = {n_label_based}, final to annotate = {len(final_ids_to_annotate)}\")\n",
    "\n",
    "            # (8) アノテーション処理\n",
    "            newly_annotated_count = 0\n",
    "            cond_correct_list = []\n",
    "\n",
    "            df_topN = df_valid[df_valid[\"track_id\"].isin(final_ids_to_annotate)].copy()\n",
    "            group_fpath = df_topN.groupby(\"gal_fpath\")[\"track_id\"].unique()\n",
    "\n",
    "            for gfpath, tids_array in group_fpath.items():\n",
    "                df_gal_chunk = pd.read_pickle(gfpath)\n",
    "                df_gal_chunk = map_pids_to_0_known(df_gal_chunk, pids_to_add,\n",
    "                                                   label_col=\"pid\", new_label_col=\"pid_trans\")\n",
    "\n",
    "                df_to_add = df_gal_chunk[\n",
    "                    (df_gal_chunk[\"tracklet_id\"].isin(tids_array)) &\n",
    "                    (df_gal_chunk[\"annotated\"] == False)\n",
    "                ].copy()\n",
    "                if not df_to_add.empty:\n",
    "                    newly_ids = df_to_add[\"tracklet_id\"].unique()\n",
    "                    newly_annotated_count += len(newly_ids)\n",
    "\n",
    "                    cond_correct_part = df_res_all[\n",
    "                        (df_res_all[\"track_id\"].isin(newly_ids)) &\n",
    "                        (df_res_all[\"correct\"] == 1)\n",
    "                    ]\n",
    "                    if not cond_correct_part.empty:\n",
    "                        cond_correct_list.append(cond_correct_part)\n",
    "\n",
    "                    max_tid = new_train[\"tracklet_id\"].max() if \"tracklet_id\" in new_train.columns else -1\n",
    "                    df_to_add[\"tracklet_id\"] = df_to_add[\"tracklet_id\"] + max_tid + 1\n",
    "\n",
    "                    new_train = pd.concat([new_train, df_to_add], ignore_index=True)\n",
    "\n",
    "                    original_idx = df_gal_chunk[\n",
    "                        (df_gal_chunk[\"tracklet_id\"].isin(newly_ids)) &\n",
    "                        (df_gal_chunk[\"annotated\"] == False)\n",
    "                    ].index\n",
    "                    df_gal_chunk.loc[original_idx, \"annotated\"] = True\n",
    "\n",
    "                    df_gal_chunk.to_pickle(gfpath)\n",
    "\n",
    "                del df_gal_chunk, df_to_add\n",
    "                gc.collect()\n",
    "\n",
    "            if cond_correct_list:\n",
    "                cond_correct_all = pd.concat(cond_correct_list, ignore_index=True)\n",
    "                correct_newly_annotated = cond_correct_all.shape[0]\n",
    "            else:\n",
    "                cond_correct_all = pd.DataFrame()\n",
    "                correct_newly_annotated = 0\n",
    "\n",
    "            print(f\"  -> newly_annotated={newly_annotated_count}, correct_newly_annotated={correct_newly_annotated}\")\n",
    "\n",
    "            if not cond_correct_all.empty:\n",
    "                grp_label = cond_correct_all.groupby(\"pid_true\").size().reset_index(name=\"correct_count\")\n",
    "                print(\"\\n=== correct_newly_annotated breakdown by pid_true ===\")\n",
    "                print(grp_label.sort_values(\"pid_true\"))\n",
    "                breakdown_str = \"; \".join(f\"{int(r.pid_true)}:{int(r.correct_count)}\"\n",
    "                                          for r in grp_label.itertuples())\n",
    "            else:\n",
    "                print(\"\\nNo newly annotated tracklets in this round -> No breakdown\")\n",
    "                breakdown_str = \"\"\n",
    "\n",
    "            # (9) ログ書き出し\n",
    "            row_dict = {\n",
    "                \"chunk\": chunk_idx+1,\n",
    "                \"round\": round_idx,\n",
    "                \"pids_to_add\": str(pids_to_add),\n",
    "                \"train_samples\": len(new_train),\n",
    "                \"newly_annotated_count\": newly_annotated_count,\n",
    "                \"correct_newly_annotated\": correct_newly_annotated,\n",
    "                \"correct_newly_annotated_breakdown\": breakdown_str\n",
    "            }\n",
    "            df_summary = pd.DataFrame([row_dict])\n",
    "            if not os.path.exists(summary_csv):\n",
    "                df_summary.to_csv(summary_csv, index=False)\n",
    "            else:\n",
    "                df_summary.to_csv(summary_csv, index=False, mode=\"a\", header=False)\n",
    "            del df_summary, row_dict\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"\\n[Chunk {chunk_idx+1} done] pids_to_add={pids_to_add}, chunk_time={(time.time()-chunk_start_t):.2f}s\")\n",
    "\n",
    "    print(\"\\n=== All Done ===\")\n",
    "    print(\"See 'summary_results_aug_matrix.csv' for chunk-by-chunk progress.\")\n",
    "    print(\"Also check the classification reports printed each round for details.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e156a28d-265f-4f35-b2dc-76c612d708b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#############################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Cell 8: Notebook上で main() を呼び出す例\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#############################################\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 実際の実行は任意です。\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# もしこのNotebookでスクリプトを一通り動かしたい場合はこちらのセルを実行してください。\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    データの読み込み、学習、推論、アノテーションフローのサイクルをまとめたメイン関数。\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    ピックアップされたクエリのトラックレットを既存学習データに追加し、\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    ArcFaceベースの学習・推論を繰り返しながらギャラリーのアノテーションを進める。\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mset_random_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# 1) トレーニングデータ読み込み\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# ===============================\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m, in \u001b[0;36mset_random_seed\u001b[1;34m(seed_value)\u001b[0m\n\u001b[0;32m     41\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed_value)\n\u001b[0;32m     42\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed_value)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(seed_value)\n\u001b[0;32m     44\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(seed_value)\n\u001b[0;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed_value)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Cell 8: Notebook上で main() を呼び出す例\n",
    "#############################################\n",
    "\n",
    "# 実際の実行は任意です。\n",
    "# もしこのNotebookでスクリプトを一通り動かしたい場合はこちらのセルを実行してください。\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194105ee-3b70-41af-871f-eb7b844fca5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
